{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction and noise removal of face images with Non-Negative Matrix Factorization\n",
    "\n",
    "\n",
    "\n",
    "***TODO***\n",
    "\n",
    "***\n",
    "- Lage plotfunksjon som kan lage ax=(1, 1) plot\n",
    "- Lage funksjon som setter sammen plot slik at man kan få 2-3 enkle ved siden av hverandre\n",
    "\n",
    "- Fikse alignment på flere bilder funksjonen. a, b, c er litt off.\n",
    "\n",
    "text = 'https://devnote.in'\n",
    "\n",
    "textwidth, textheight = draw.textsize(text)\n",
    "***\n",
    "\n",
    "- Kjøre den siste simuleringen over natta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Introduction](#intro)\n",
    "* [Theory behind methods](#method)\n",
    "* [Analysing the CryptoPunks dataset](#dataset)\n",
    "* [Denoising](#denoising)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "\\\n",
    "\\\n",
    "\n",
    "An important subset of machine learning is dimentionality reduction. \n",
    "\n",
    "Examples include ...\n",
    "\n",
    "One method of dimentionality reduction is non-negative matrix factorization (referred to as NMF throughout his notebook). In this project we will investigate this method. We begin by implementing it and testing on simple matrices, before using our tools to reduce the dimentionality of an image dataset. Towards the end of the notebook we attempt denoising with the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "*A note on runtime etc...*\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n",
    "Below we import the libraries needed for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for math and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Image handling, files, timing and status bar\n",
    "#%pip install opencv-python \n",
    "# Required to import cv2 !!!\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Gives a progress bar \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Used to combine saved images\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the amount of lines required to make plots, and avoid copying similar code to many cells, we create a standard plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(x_vals_list, y_vals_list, linestyles, shape = False, fig_size = (15, 5), \n",
    "    plot_type = \"not hist\",\n",
    "    xlabels = [\"\" for i in range(20)], ylabels = [\"\" for i in range(20)],\n",
    "    fig_text = False, fig_text_size = 15, fig_text_y = -0.04, \n",
    "    title_size = 15, titles = [\"\" for i in range(20)], \n",
    "    label_size = 15, labels = [False for i in range(20)], loc = [\"upper right\" for i in range(20)]):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Create fig, axs\n",
    "    if not shape:\n",
    "        shape = (1, len(x_vals_list))\n",
    "    fig, axs = plt.subplots(shape[0], shape[1], figsize=fig_size)\n",
    "        \n",
    "    # Make axs iterable if only 1\n",
    "    if len(x_vals_list) == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    # Reshape to 1D list of ax if grid-shaped\n",
    "    if shape[0] != 1 and shape[1] != 1:\n",
    "        axs = [axs[i//shape[0], i%shape[0]] for i in range(shape[0]*shape[1])]\n",
    "\n",
    "    # Loop and plot\n",
    "    for i, (ax, x, y) in enumerate(zip(axs, x_vals_list, y_vals_list)):\n",
    "        for xvals, yvals, style in zip(x, y, linestyles[i]):\n",
    "            if plot_type == \"hist\":\n",
    "                ax.hist(yvals)\n",
    "            else:\n",
    "                ax.plot(xvals, yvals, style)\n",
    "        ax.set_title(titles[i], size = title_size)\n",
    "        if labels[i]:\n",
    "            ax.legend(labels[i], loc=loc[i], prop={'size': label_size-2})\n",
    "        ax.set_xlabel(xlabels[i], size = label_size)\n",
    "        ax.set_ylabel(ylabels[i], size = label_size)\n",
    "    \n",
    "    \n",
    "    if fig_text:\n",
    "        plt.figtext(0.5, fig_text_y, fig_text, wrap=True, horizontalalignment='center', fontsize = fig_text_size)\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TASK 1) Theory behind methods <a class=\"anchor\" id=\"method\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Non-negative matrix factorization (NMF) is a method for reducing the dimension of matrices, given by: \n",
    "\n",
    "$$A ≈ A_{NMF} = W H$$\n",
    "\n",
    "Where $A$ is a $m\\,\\times\\,n$ matrix that we want to reduce the dimensionality of. $W$ is a non negative $m\\,\\times\\,d$ matrix and $H$ is a non negative $d\\,x\\,n$ matrix. $d$ is usually chosen to be considerably smaller than $n$ and $m$, so that $A$ can be represented in a more compact manner, but if $d$ is too small it can lead to an inadequate apptroximation of $A$. We will refer to $WH$ as the reconstruction of $A$.\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "***MORE about how to interpret the values in W and H. Extra insight, applications for spotify etc..., ..., \"in out case\"?***\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n",
    "The NMF of $A$ satisfies:\n",
    "\n",
    "$$\\min_{W, H} \\lVert A - WH \\rVert_F $$\n",
    "\n",
    "such that $W$ and $H$ are non negative. There are infinetly many combinations of $W$ and $H$ that satisfy this condition, meaning that the NMF is non unique. We will use an iterative algorithm to calculate the NMF given by:\n",
    "\n",
    "$$H_{k+1} ← H_k \\odot (W^T_k A) \\oslash (W^T_k W_kH_k)$$\n",
    "\n",
    "$$W_{k+1} ← W_k \\odot (AH^T_{k+1}) \\oslash (W_kH_{k+1}H^T_{k+1})$$\n",
    "\n",
    "This algortihm satisfies: \n",
    "\n",
    "$$\\lVert A - W_{k+1}H_{k+1} \\rVert_F \\leqslant \\lVert A - W_kH_k \\rVert_F$$\n",
    "\n",
    "Meaning that it converges to a local minimizer of the error, and therefore it may just find and approximate solution of the NMF, which again is an approximation of $A$. As a result there is a big room for error as the NMF might be a poor approximation of $A$ and our algorithm might not converge to a satisfying solution for the NMF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) \n",
    "    \"We will show that if $A$ is non negative and $W$ and $H$ are initialized as positive, then all iterates $W_k$ and $H_k$ are also positive. This will by done by induction:  **Hvis a er 0 blir vel W_k og H_k også 0?, så vil må vel endre formulerinen litt?**\\n\",\n",
    "    \"\\n\",\n",
    "    \"Asssume $A$ is non negative and $W$ and $H$ are initialized as positive, and let $P(n)$ be the statement $W_k$ and $H_k$ are positive.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The hypothesis clearly holds for the base case $P(0)$, as $W_0=W$ and $H_0=H$ being positive is part of our assumptions. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we assume the induction hypothesis holds for $n=k$. We have that:\\n\",\n",
    "    \"\\n\",\n",
    "    \"$H_{k+1} ← H_k ⊙ (W^T_k A) ⊘ (W^T_k W_kH_k)$\\n\",\n",
    "    \"\\n\",\n",
    "    \"$W_{k+1} ← W_k ⊙ (AH^T_{k+1}) ⊘ (W_kH_{k+1}H^T_{k+1})$\\n\",\n",
    "    \"\\n\",\n",
    "    \"It is clear that $H_{k+1}$ is positive, as it is a product of $H_k$, $W_k$, $W^T_k$ and $A$, which are all positive under our assumptions and induction hypothesis. \\n\",\n",
    "    \"\\n\",\n",
    "    \"It follows that $W_{k+1}$ is positive, as it is also a product of only positive matrices. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Hence the induction hypothesis holds for $n=k+1$. By induction it follows that $P(n)$ holds for all $n$ in $\\mathbb{Z^+}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) \n",
    "\n",
    "If $(W^T_k A) ⊘ (W^T_k W_kH_k)$ is a matrix of ones we see that when calculating $H_{k+1}$ we multiply every element of $H_k$ by $1$, meaning $H_{k+1}=H_k$, this is known as a fixed point. This happens when $W_kH_k=A$: \n",
    "\n",
    "$$(W^T_k A) ⊘ (W^T_k W_kH_k)=(W^T_k A) ⊘ (W^T_k A).$$\n",
    "\n",
    "We know that ⊘ denotes elementvise division, so we are left with a matrix of the elements in $W^T_k A$ divided by themselves, that is clearly a matrix of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) vise blabla\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\\\n",
    "\n",
    "If either $W_0$ or $H_0$ are set to matrices containing only zeros, we can see from **ref Hk+1 Wk+1** that $H_k$ and $W_k$ converges to $0$ regardless of $A$. In other words $WH$ will never converge towards $A$, unless by chance $A$ is also a matrix of zeros. In out implementation we therefore radomise the initial values from the uniform dirstribution between 0 and 1, $U[0, 1]$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation and testing**\n",
    "\n",
    "We will now implement the NMF algorithm. To avoid division by 0 we will include a safe division constant $\\delta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d, e, f\n",
    "\n",
    "def NMF(V, d, delta = 1e-9, maxiter = 1000, seed = 4269):\n",
    "    \"\"\"\n",
    "    Performs maxiter iterations of the NMF algorithm described above to find \n",
    "    matrices W and H such that V ≈ WH. \n",
    "    input:\n",
    "        V: (m,n) input array\n",
    "        d: integer, Number of components we want to decompose V into\n",
    "        delta, float, small number for safe division\n",
    "        maxiter: integer, maximum number of iterations\n",
    "        seed: integer, random seed\n",
    "    output:\n",
    "        W: (m,d) array\n",
    "        H: (d,n) array\n",
    "        err: array of lenght maxiter+1 with Frodenius norms ||V-W_k@H_k||\n",
    "    \"\"\"\n",
    "\n",
    "    if seed != 0:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    # Find dimensions of V\n",
    "    m = np.shape(V)[0]\n",
    "    n = np.shape(V)[1]\n",
    "    \n",
    "    # Initialize W and H with numbers from Unif(0, 1), and rescale by factor sqrt(mean(A)/d)\n",
    "    random_numbers_01 = np.random.uniform(0.0, 1.0, m*d+d*n) # we need m*d+d*n numbers\n",
    "    W = np.sqrt(np.mean(V)/d) * random_numbers_01[0:m*d].reshape((m, d))\n",
    "    H = np.sqrt(np.mean(V)/d) * random_numbers_01[m*d:m*d+d*n].reshape((d, n))\n",
    "\n",
    "    # Initialize the array err for storing the Frobenius norm ||V-WH||\n",
    "    err    = np.zeros(maxiter+1)\n",
    "    err[0] = np.linalg.norm(V - np.dot(W, H), 'fro')\n",
    "    \n",
    "    # Perform the iterations\n",
    "    for k in tqdm(range(maxiter), leave = False):\n",
    "        H *= np.dot(W.T, V) / (np.dot(W.T, np.dot(W, H)) + delta)\n",
    "        W *= np.dot(V, H.T) / (np.dot(W, np.dot(H, H.T)) + delta)\n",
    "        err[k+1] = np.linalg.norm(V - np.dot(W, H), 'fro')\n",
    "    \n",
    "    return W, H, err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that our algorithm works we define the following matrices:\n",
    "\n",
    "$$ \n",
    "A_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix},\\,\\,\\,\\,\\,\n",
    "A_2 = \\begin{bmatrix} 1 & 2 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix},\\,\\,\\,\\,\\,\n",
    "A_3 = \\begin{bmatrix} 2 & 1 & 1 \\\\ 2 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix},\\,\\,\\,\\,\\,\n",
    "A_4 = \\begin{bmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 3 \\\\ 0 & 3 & 3 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We begin by running the algorithm with $d=1$ for $A_1$ and $A_2$ for two differend seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.array([[1, 0],\n",
    "               [0, 1]], dtype = \"float\")\n",
    "\n",
    "A2 = np.array([[1, 2],\n",
    "               [1, 1],\n",
    "               [1, 2]], dtype = \"float\")\n",
    "\n",
    "def A1A2_test(seeds, d = 1, dec = 3):\n",
    "    for seed in seeds:\n",
    "        W1, H1, err1 = NMF(A1, d, seed = seed)\n",
    "        print(f\"With A = A\\u2081 =\\n{np.round(A1, dec)}, \\nwe find W = \\n{np.round(W1, dec)},\",\n",
    "              f\"\\nH = \\n{np.round(H1, dec)}, \\nWH = \\n{np.round(np.dot(W1, H1), dec)}.\")\n",
    "        print(f\"This gives the Frobenius norm ||A\\u2081-WH|| = {err1[-1]}\")\n",
    "        print(100*\"-\")\n",
    "\n",
    "    for seed in seeds:\n",
    "        W2, H2, err2 = NMF(A2, d, seed = seed)\n",
    "        print(f\"With A = A\\u2082 =\\n{np.round(A2, dec)}, \\nwe find W = \\n{np.round(W2, dec)},\",\n",
    "              f\"\\nH = \\n{np.round(H2, dec)}, \\nWH = \\n{np.round(np.dot(W2, H2), dec)}.\")\n",
    "        print(f\"This gives the Frobenius norm ||A\\u2082-WH|| = {err2[-1]}\")\n",
    "\n",
    "        print(100*\"-\")\n",
    "    \n",
    "seeds = [42, 69]\n",
    "# A1A2_test(seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncommenting the bottom line yields the results presented in the table below. \n",
    "\n",
    "|                |        $A$        |       $W$       |       $H$       |       $WH$      |$\\lVert{A-WH}\\rVert_F$|\n",
    "|:-----------------:|:-----------------:|:-----------------:|:-----------------:|:-----------------:|:-----------------:|\n",
    "| seed 1 | \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} | \\begin{bmatrix} 0.265 \\\\ 0.672 \\end{bmatrix} | \\begin{bmatrix} 0.507 & 1.288 \\end{bmatrix} | \\begin{bmatrix} 0.134 & 0.341 \\\\ 0.341 & 0.866 \\end{bmatrix} | $1.0$ |  \n",
    "| seed 2 |                                              | \\begin{bmatrix} 0.209 \\\\ 0.572 \\end{bmatrix} | \\begin{bmatrix} 0.564 & 1.541 \\end{bmatrix} | \\begin{bmatrix} 0.118 & 0.323 \\\\ 0.323 & 0.882 \\end{bmatrix} | $1.0$ \n",
    "| seed 1 | \\begin{bmatrix} 1 & 2 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix} | \\begin{bmatrix} 1.078 \\\\ 0.658 \\\\ 1.078 \\end{bmatrix} | \\begin{bmatrix} 1.021 & 1.803 \\end{bmatrix} | \\begin{bmatrix} 1.1 & 1.943 \\\\ 0.671 & 1.186 \\\\ 1.1 & 1.943 \\end{bmatrix} | $0.411$ |\n",
    "| seed 2 |                                               | \\begin{bmatrix} 0.869 \\\\ 0.530 \\\\ 0.869 \\end{bmatrix} | \\begin{bmatrix} 1.266 & 2.236 \\end{bmatrix} | \\begin{bmatrix} 1.1 & 1.943 \\\\ 0.671 & 1.186 \\\\ 1.1 & 1.943 \\end{bmatrix} | $0.411$ | \n",
    "\n",
    "***DISCUSS***\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\what is same\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "# A1A2_test([4269], d = 2, dec = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e)\n",
    "\n",
    "|        $A$        |       $W$       |       $H$       |       $WH$      |$\\lVert{A-WH}\\rVert_F$|\n",
    "|:-----------------:|:-----------------:|:-----------------:|:-----------------:|:-----------------:|\n",
    "| \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} | \\begin{bmatrix} 0.11 & 0 \\\\ 0 & 0.132 \\end{bmatrix} | \\begin{bmatrix} 9.109 & 0 \\\\ 0 & 7.567 \\end{bmatrix} | \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} | $1.718\\cdot10^{-10}$ |  \n",
    "| \\begin{bmatrix} 1 & 2 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix} | \\begin{bmatrix} 0.265 & 0.38 \\\\ 0.079 & 0.463 \\\\ 0.265 & 0.38 \\end{bmatrix} | \\begin{bmatrix} 0.892 & 5.871 \\\\ 2.007 & 1.163 \\end{bmatrix} | \\begin{bmatrix} 1 & 2 \\\\ 1 &  1 \\\\ 1 & 2 \\end{bmatrix} | $6.589\\cdot 10^{-10}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f FIND ranks\n",
    "A3 = np.array([[2, 1, 1],\n",
    "               [2, 1, 1],\n",
    "               [1, 1, 2]], dtype = \"float\")\n",
    "\n",
    "A4 = np.array([[2, 1, 0],\n",
    "               [1, 2, 3],\n",
    "               [0, 3, 3]], dtype = \"float\")\n",
    "\n",
    "print(f\"\\u03C3(A\\u2083) = \\u007b{np.linalg.eig(A3)[0][0]}, {np.linalg.eig(A3)[0][1]}, {np.linalg.eig(A3)[0][2]}\\u007d\")\n",
    "print(f\"\\u03C3(A\\u2084) = \\u007b{np.linalg.eig(A4)[0][0]}, {np.linalg.eig(A4)[0][1]}, {np.linalg.eig(A4)[0][2]}\\u007d\\n\")\n",
    "\n",
    "print(f\"np.linalg.matrix_rank(A3) returns {np.linalg.matrix_rank(A3)}\")\n",
    "print(f\"np.linalg.matrix_rank(A4) returns {np.linalg.matrix_rank(A4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the space of eigenvalues, $\\sigma(A_3)$, we see that 0 is an eigenvalue of the matrix $A_3$. It only has two non-zero eigenvalues, and is therefore a rank $2$ matrix. $\\sigma(A_4)$ shows that $A_4$ has three non-zero eigenvalues, so it is a rank 3 matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g\n",
    "\n",
    "\"\"\"\n",
    "g) For A3 and A4, run the NMF algorithm for d = 1, d = 2 and d = 3 and\n",
    "plot ∥A − WkHk∥F as a function of number of iterations in two separate plots\n",
    "(one for A3 and one for A4). Scale the y-axis logarithmically, using for example\n",
    "plt.semilogy. Is equation (17) satisfied? How do the results depend on d?\n",
    "\"\"\"\n",
    "\n",
    "d_vals = [1, 2, 3]\n",
    "A3_error_arrays = []\n",
    "A4_error_arrays = []\n",
    "for d in d_vals:\n",
    "    A3_error_arrays.append(NMF(A3, d)[2])\n",
    "    A4_error_arrays.append(NMF(A4, d)[2])\n",
    "\n",
    "\n",
    "    \n",
    "# ### DETTE skal gjøres om til en generell plotfunksjon\n",
    "# fig, axs = plt.subplots(1, 2, figsize = (15, 4))\n",
    "# for i in range(len(d_vals)):\n",
    "#     axs[0].plot(np.log10(A3_error_arrays[i]), label = r\"$d$ = \" + f\"{d_vals[i]}\")\n",
    "#     axs[1].plot(np.log10(A4_error_arrays[i]), label = r\"$d$ = \" + f\"{d_vals[i]}\")\n",
    "\n",
    "# axs[0].set_title(r\"Using matrix $A_3$\")\n",
    "# axs[1].set_title(r\"Using matrix $A_4$\")\n",
    "\n",
    "# axs[0].legend(loc = \"upper right\")\n",
    "# axs[1].legend(loc = \"upper right\")\n",
    "\n",
    "# axs[0].set_xlabel(r\"Iterations, $k$\")\n",
    "# axs[0].set_ylabel(r\"log $||A-WH||_F$\")\n",
    "# axs[1].set_xlabel(r\"Iterations, $k$\")\n",
    "# axs[1].set_ylabel(r\"log $||A-WH||_F$\")\n",
    "\n",
    "\n",
    "# figure_count += 1\n",
    "# txt = f\"$Figure$ ${figure_count}$: \" + r\"log of Frobenius norm $||A-WH||_F$ for different d.\"\n",
    "# plt.figtext(0.5, -0.1, txt, wrap=True, horizontalalignment='center', fontsize = 16)\n",
    "# plt.show()\n",
    "\n",
    "x_vals_list = [[np.linspace(0, 1000+1, 1000+1) for i in range(3)] for j in range(2)]\n",
    "linestyles  = [[\"black\", \"blue\", \"green\"] for i in range(2)]\n",
    "fig_text    = \"Figure XX: fdsdfdgs\"\n",
    "\n",
    "plot_function(x_vals_list, [A3_error_arrays, A4_error_arrays], linestyles, fig_size = (15, 10), \n",
    "        plot_type = \"not hist\", shape = (2, 1),\n",
    "        xlabels = [\"\", r\"$t$ [s]\"], ylabels = [r\"log $||A-WH||_F$\" for i in range(2)],\n",
    "        fig_text=fig_text, fig_text_size = 15, fig_text_y = 0.04, \n",
    "        title_size = 15, titles = [r\"$A_3$\", \"$A_4$\"], \n",
    "        label_size = 15, labels = [False, False], loc = [\"upper right\", \"upper right\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we see ...\n",
    "\n",
    "Satisfy ***REF***\n",
    "\n",
    "$$\\lVert A - W_{k+1}H_{k+1} \\rVert_F \\leqslant \\lVert A - W_kH_k \\rVert_F$$\n",
    "\n",
    "***Discuss d dependence***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now developed the tools necessary to begin exploring a larger dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TASK 2) Analysing the CryptoPunks dataset <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now turn our attention to image ... ...\n",
    "\n",
    "***EXPLAin how images are stored +++, page 13 in task***\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this project consists of 10 000 images. Of these we will sample $N=500$ random images to ensure we get unique results. We use a predeterminded seed to ensure the results won't change if the notebook is restarted. The load_images function was handed out, but has been modified slightly to pick images before loading them into the notebook. ***LATER WE WILL VARY N???*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(N, seed = 4269):\n",
    "    \"\"\"\n",
    "    Loads N random images from cryptopunk dataset. Use the same seed to get the same\n",
    "    selection each time. \n",
    "    Input:\n",
    "        integer: N, number of images to load\n",
    "        integer: seed for random sampling\n",
    "    Output:\n",
    "        array: faces, (24,24,4,N) numpy array containing images\n",
    "    \"\"\"\n",
    "    # Allocate array to store images\n",
    "    faces = np.zeros((24,24,4,N))\n",
    "\n",
    "    # Chose N random image numbers\n",
    "    M = 10000 # number of images to choose from\n",
    "    np.random.seed(seed)\n",
    "    image_number_choice = np.random.choice(np.arange(0, M),N, replace = False)\n",
    "        \n",
    "    # Iterate over folders\n",
    "    for i in range(len(image_number_choice)):\n",
    "        im_path = f\"./imgs/imgs/imgs/{image_number_choice[i]}.png\"\n",
    "        im = cv2.imread(im_path, cv2.IMREAD_UNCHANGED)\n",
    "        faces[:,:,:,i] = cv2.cvtColor(im, cv2.COLOR_BGRA2RGBA)/255.0\n",
    "\n",
    "    return faces\n",
    "\n",
    "# Load N = 500 faces\n",
    "faces500 = load_images(500)\n",
    "\n",
    "# Check that the shape is correct\n",
    "print(\"The face array has shape: \", faces500.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is handed-out code for plotting the images we have loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotimgs(imgs, nplot = 8, rescale = False, filename = None, figsize = (16, 16), number_color = \"black\", show = True):\n",
    "    \"\"\"\n",
    "    Plots nplot*nplot images on an nplot x nplot grid. \n",
    "    Saves to given filename if filename is given\n",
    "    Can also rescale the RGB channels\n",
    "    input:\n",
    "        imgs: (24,24,4,N) or (24,24,3,N) array containing images, where N > nplot**2\n",
    "        nplot: integer, nplot**2 images will be plotted\n",
    "        rescale: bool\n",
    "        filename: string, figure will be saved to this location. Should end with \".png\".\n",
    "    \"\"\"\n",
    "    # Initialize subplots\n",
    "    fig, axes = plt.subplots(nplot, nplot, figsize = figsize)\n",
    "\n",
    "    # Set background color\n",
    "    plt.gcf().set_facecolor(\"lightgrey\")\n",
    "\n",
    "    # Iterate over images\n",
    "    for idx in range(nplot**2):\n",
    "        \n",
    "        # Indices\n",
    "        i = idx//nplot; j = idx%nplot\n",
    "\n",
    "        # Remove axis\n",
    "        if nplot > 1:\n",
    "            axes[i,j].axis('off')\n",
    "        else:\n",
    "            axes.axis('off')\n",
    "\n",
    "        # Rescale RGB channels by dividing my maximal value\n",
    "        if rescale:\n",
    "            scaled_img = np.copy(imgs[:,:,:,idx])\n",
    "            scaled_img[:,:,:3] = scaled_img[:,:,:3]/np.max(scaled_img[:,:,:3])\n",
    "            if nplot > 1:\n",
    "                axes[i,j].imshow(scaled_img)\n",
    "            else:\n",
    "                axes.imshow(scaled_img)\n",
    "        else:\n",
    "            if nplot > 1:\n",
    "                axes[i,j].imshow(imgs[:,:,:,idx])\n",
    "            else:\n",
    "                axes.imshow(imgs[:,:,:])\n",
    "        \n",
    "        # Add numbers to each image\n",
    "        if nplot > 1 and number_color:\n",
    "            axes[i, j].text(0, 4, f\"{idx+1}\", color = number_color, fontsize = 20)\n",
    "\n",
    "    # Tight layout so images will appear closer together\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if filename is given\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def plotimgs_merge(filetitles, merged_title, img_txt = [\"a)\", \"b)\", \"c)\"]):\n",
    "    \"\"\"\n",
    "    Plots images side by side. This lets us combine images and serves as a workaround to \n",
    "    jupyters limitation of one image attachment per markdown cell.\n",
    "    input:\n",
    "        filetitles: list of titles of images to plot\n",
    "    \"\"\"\n",
    "    num_of_imgs = len(filetitles) # Number of images\n",
    "    size = 200 # x and y dimension of images produced with figsize (16, 16)\n",
    "\n",
    "    # Load images\n",
    "    images = [Image.open(filetitle + \".png\") for filetitle in filetitles]\n",
    "    \n",
    "    # Resize and merge the images\n",
    "    x_size = int(size*(1+1/20))*num_of_imgs + int(size*(1/20))\n",
    "    y_size = int(size*(1+3/20)) # double space below for figtext\n",
    "    \n",
    "    merged_Image = Image.new('RGB', (x_size, y_size), (250, 250, 250))\n",
    "    for index, image in enumerate(images):\n",
    "        # Resize image\n",
    "        image = image.resize((size, size))\n",
    "        \n",
    "        # Place image in merged image\n",
    "        x_coord = int(size*(1+1/20))*index + int(size*(1/20))\n",
    "        y_coord = int(size*(2/20))\n",
    "        merged_Image.paste(image, (x_coord, y_coord))\n",
    "        \n",
    "    # Add text above each image:\n",
    "    draw = ImageDraw.Draw(merged_Image)\n",
    "    for index, text in enumerate(img_txt):\n",
    "        x_coord = int(size*(1/20) + size/2) + index*(size*(1+1/20)) - draw.textsize(text)[0]/2\n",
    "        y_coord = int(size*(1/20))\n",
    "        draw.text((x_coord, y_coord), text, (0,0,0))\n",
    "    \n",
    "    # Save image\n",
    "    merged_Image.save(merged_title + \".png\",\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First look at the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) We now have all we need to investigate ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotimgs(faces500, nplot = 8, filename=\"2a_punks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above gives the image below. \n",
    "\n",
    "<img src=punks_2a.png alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We observe ... \n",
    "\n",
    "COMMENTs on 2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting the dataset**\n",
    "\n",
    "To apply NMF we want to turn our (24, 24, 4, 500) array into a $m\\times n$ matrix. For this project we only care about the color channels. We will therefore split the RGBA images into a RGB array of shape (24, 24, 3, 500) and a opacity array (24, 24, 1, 500) wich we store for the plotting the reconstructed images after running out algorithm. Below we define a function to split the channels and reshape the RGB array into a $1728\\times N$ matrix. We also define a function to reshape a $1728\\times N$ back into an RGB array, and optionaly remerge with the opacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_reshape(faces):\n",
    "    #input: 24x24x4xN array of stored images\n",
    "    #output: 1728xN array of with rgb values for the images without opacity, 24x24x500 array of opacity for images \n",
    "    faces_opacity = faces[:,:,3,:]\n",
    "    faces_rgb = faces[:,:,:3,:]\n",
    "    shape = faces_rgb.shape\n",
    "    \n",
    "    # Clever reshape \"trick\". This is the matrix we will apply the NMF to\n",
    "    N = faces.shape[-1]\n",
    "    faces_reshaped = faces_rgb.reshape(np.prod(shape)//N, N)\n",
    "    \n",
    "    return faces_reshaped, faces_opacity\n",
    "\n",
    "def merge_and_reshape(reshaped, opacity = np.array([])):\n",
    "    #input: 1728xN array of with rgb values for the images without opacity, 24x24xN array of opacity for images \n",
    "    #output: 24x24x4xN array of images\n",
    "    N = int(reshaped.size/1728)\n",
    "    faces = np.reshape(reshaped, (24,24,3,N))\n",
    "    if opacity.size > 0:\n",
    "        opacity = np.reshape(opacity, (24,24,1,N))\n",
    "        faces = np.concatenate((faces, opacity), axis=2)\n",
    "    \n",
    "    faces = np.where(faces<=1, faces, 1)\n",
    "    \n",
    "    return faces\n",
    "\n",
    "# We reshape the faces with our function\n",
    "faces500_reshaped, opacity500 = split_and_reshape(faces500)\n",
    "\n",
    "# Check that the shape is correct\n",
    "print(\"The reshaped face array has shape: \", faces500_reshaped.shape)\n",
    "\n",
    "# Test the merge_and_reshape function\n",
    "faces500_merge_test = merge_and_reshape(faces500_reshaped, opacity500)\n",
    "print(\"The remerged face array has shape: \", faces500_merge_test.shape)\n",
    "# plotimgs(faces_merge_test, 8, filename=\"merge_testpunks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The *average* face**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) We first ... calculate mean ... usefull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"np.linalg.matrix_rank(faces500_reshaped) returns {np.linalg.matrix_rank(faces500_reshaped)}\\n\")\n",
    "\n",
    "### np.linalg.svd(faces500_reshaped) SKAL vi fikse den? \n",
    "# https://mattelab2022v.math.ntnu.no/t/project-3-task-2b/1212/2\n",
    "\n",
    "\"\"\"\n",
    "To actually find the rank you can apply the SVD (np.linalg.svd) and count the number of non-zero singular values (after rounding off! For example, count only the singular values that are larger than \n",
    "10\n",
    "−\n",
    "12\n",
    " or some other small number). This is more or less the invertible matrix theorem from Matte 3/linear algebra. A rank \n",
    "r\n",
    " matrix has \n",
    "r\n",
    " non-zero eigenvalues if the matrix is square or singular values if the matrix is non-square.\n",
    "\n",
    "You can also just use something like np.linalg.matrix_rank, but you should use the SVD to show that you can apply the theory\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#2b) The mean face\n",
    "\n",
    "# Find and plot the average face\n",
    "mean_face = np.mean(faces500, axis = -1)\n",
    "plotimgs(mean_face, 1, filename=\"2b_meanface.png\", figsize = (3, 3))\n",
    "\n",
    "# TODO: OLA KOMMENTER DENNE\n",
    "mean_face[:,:,0] = mean_face[:, :, 3] == 0\n",
    "mean_face[:,:,1] = mean_face[:, :, 3] == 0\n",
    "mean_face[:,:,2] = mean_face[:, :, 3] == 0\n",
    "mean_face[:, :, 3] = 1\n",
    "\n",
    "plotimgs(mean_face, 1, filename=\"2b_0avg.png\", figsize = (3, 3)) # hvit betyr her gjennomsnitt lik 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) \n",
    "\n",
    "Useful info?\n",
    "\n",
    "Pixels with no opacity for all?\n",
    "\n",
    "Rank of image matrix???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF with $d=64$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c)\n",
    "\n",
    "***c) Calculate the NMF of the 3 color channels as explained above using d = 64. Plot\n",
    "the columns of W interpreted as RGB images (each columns should be a 24×24×3\n",
    "vector so you can use the handed out plotting function after reshaping). Explain\n",
    "and discuss what you see. Does the NMF capture the important features of the\n",
    "dataset? You can also try with different values for d to see how this affects the\n",
    "results.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2c, 2d\n",
    "\n",
    "def reconstruction_images(d, maxiter = 1000, N = 500, basis_color = False, faces_to_plot=8, show=True, brightness = 1):\n",
    "    # Load and Reshape\n",
    "    faces_reshaped_d, opacity_d = split_and_reshape(load_images(N))\n",
    "    \n",
    "    # Perform NMF algorithm\n",
    "    W_d, H_d, err_d = NMF(faces_reshaped_d, d, maxiter = maxiter)\n",
    "    \n",
    "    # Reshape and plot basis\n",
    "    if d > 1:\n",
    "        W_columns_d = merge_and_reshape(W_d*brightness) # Any scaling of W is valid as we can scale H by 1/brightness \n",
    "        plotimgs(W_columns_d, int(d**(1/2)), filename=f\"punks_basis_d{d}.png\", number_color = basis_color, show=show)\n",
    "\n",
    "    # Reshape and plot faces\n",
    "    img_reconstructed_d = merge_and_reshape(np.dot(W_d, H_d), opacity_d)\n",
    "    plotimgs(img_reconstructed_d, faces_to_plot, filename=f\"{faces_to_plot**2}punks_d{d}.png\", show=show)\n",
    "    \n",
    "# reconstruction_images(8**2, basis_color = \"white\")\n",
    "\n",
    "n = 4\n",
    "d_vals = {16, 64, 144}\n",
    "for d in d_vals:\n",
    "    reconstruction_images(d, faces_to_plot=n, show=False)\n",
    "\n",
    "plotimgs_merge([f\"punks_basis_d{d}\" for d in d_vals], \"basises\",               img_txt = [f\"d = {d}\" for d in d_vals])\n",
    "plotimgs_merge([f\"{n**2}punks_d{d}\" for d in d_vals], f\"multiple_{n**2}punks\", img_txt = [f\"d = {d}\" for d in d_vals])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above ...\n",
    "\n",
    "<img src=punks_basis_d64.png alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "We also tried different values for $d$:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global and local minima**\n",
    "\n",
    "***Vi bør kanskje si noe om vi tror vi treffer global??***\n",
    "\n",
    "\\\n",
    "\n",
    "\\\\\n",
    "\\\n",
    "\\\n",
    "\n",
    "SKAL VI KJØRE 1000 seeds??\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KUNNE vi brukt en bedre seed??\n",
    "\n",
    "def seed_test(number_of_seeds, maxiter = 1000, d = 64, N = 500, seed = 4269, data_title = \"seed_test\"):\n",
    "    # Load and split faces\n",
    "    faces_reshaped_s = split_and_reshape(load_images(N))[0] # index 1 is the opacity channel\n",
    "    \n",
    "    # Prepare array of seeds\n",
    "    np.random.seed(seed)\n",
    "    seeds = np.random.choice(range(number_of_seeds*10), number_of_seeds, replace = False)\n",
    "    \n",
    "    # Initialise list to store the different reconstructions errors\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    # Do NMF simulation and append reconstruction for each seed\n",
    "    for i in tqdm(range(number_of_seeds)):\n",
    "        W_s, H_s, err_s = NMF(faces_reshaped_s, d, maxiter = maxiter, seed = seeds[i])\n",
    "        reconstruction_errors.append(err_s[-1])\n",
    "    \n",
    "    # Save the data\n",
    "    np.savez(data_title, seeds = seeds, reconstruction_errors = reconstruction_errors)\n",
    "\n",
    "n = 100\n",
    "# seed_test(n, data_title = f\"{n}seeds.npz\")\n",
    "reconstruction_errors = np.load(f\"{n}seeds.npz\")[\"reconstruction_errors\"]\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(reconstruction_errors)\n",
    "print(min(reconstruction_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c) important features\n",
    "\n",
    "\n",
    "<img src=basises.png alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2d) \n",
    "\n",
    "***d) With d = 64, calculate the 64 reconstructions W H corresponding to the images\n",
    "you plotted in 2a) and plot the reconstructions (again, you can use the handed out\n",
    "plotting function after reshaping). Are the reconstructions overall good? How do\n",
    "the reconstructions deviate from the original images (if they deviate at all)? In\n",
    "particular, are all features of the images equally well reconstructed?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Reconstructing the dataset**\n",
    "\n",
    "<img src=multiple_16punks.png alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergence**\n",
    "\n",
    "2e) \n",
    "\n",
    "***e) For each iteration k, calculate ∥A−WkHk∥F and plot it as a function of iterations\n",
    "similarily to what you did in Task 1, but now for d = 16, 32, 64. Are the results\n",
    "reasonable? Does it look like the algorithm has converged?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2e) \n",
    "\n",
    "def error_sim(faces_reshaped, maxiter = 1000, d_vals = [16, 32, 64]):\n",
    "    image_error_arrays = []\n",
    "    for d in tqdm(d_vals, leave = False):\n",
    "        image_error_arrays.append(NMF(faces_reshaped, d, maxiter = maxiter)[2]) # øke maxiter\n",
    "    \n",
    "    txt = f\"$Figure XX: log of Frobenius norm $||A-WH||_F$ for different d for ........\"\n",
    "    plot_function([[np.arange(1, maxiter+2) for d in d_vals]], [image_error_arrays], [[\"tab:blue\", \"tab:orange\", \"tab:green\"]], \n",
    "        fig_size=(15, 5), xlabels=[\"iterations, k\"], ylabels=[r\"log $||A-WH||_F$\"],\n",
    "        fig_text=txt, labels = [[f\"$d$ = {d}\" for d in d_vals]])\n",
    "\n",
    "error_sim(faces500_reshaped, maxiter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2e) discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2f) Reconstruction error and feature number $d$**\n",
    "\n",
    "We will now do NMF calculation for a wide range of $d$ to investigate how the reconstruction error, $\\lvert\\lvert A - WH \\rvert\\rvert_F$. We expect that the error is reduced as we capture more features in matrix $W$. \n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n",
    "***f) For a wide range of d, for example d = 16, 64, 128, 256, do an NMF and calculate\n",
    "∥A−W H∥F\n",
    "9\n",
    ". Make sure you are using a high number of maximum iterations (at\n",
    "least 1000 iterations), and if your computing resources and time allows it, try to\n",
    "use more values for d. Plot this quantity as a function of d. Discuss the resulting\n",
    "plot. Specifically, how would you expect ∥A − W H∥F to depend on d?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2f)\n",
    "\n",
    "def final_error_sim(d_vals, N = 500, maxiter = 1000, noise_level = 0, data_title = False):\n",
    "    \n",
    "    # Load, reshape and add noise to images\n",
    "    faces_es          = load_images(N)\n",
    "    faces_reshaped_es = split_and_reshape(faces_es)[0]\n",
    "    faces_r_noisy_es  = split_and_reshape(faces_es)[0] # We need faces_reshaped_es for error comparison\n",
    "    \n",
    "    # We add this possibility to reuse the function later. \n",
    "    if noise_level != 0:\n",
    "        faces_r_noisy_es = add_noise(faces_reshaped_es, noise_level)\n",
    "    \n",
    "    # Initialise arrays for final errors\n",
    "    final_error_es = np.zeros(len(d_vals))\n",
    "\n",
    "    # We do the simulation for all the d values\n",
    "    for i in tqdm(range(len(d_vals)), leave = False):\n",
    "        W_es, H_es, err_es = NMF(faces_r_noisy_es, d_vals[i], maxiter = maxiter)\n",
    "        final_error_es[i]  = np.linalg.norm(faces_reshaped_es - np.dot(W_es, H_es), 'fro')\n",
    "    \n",
    "    ## SAVE DATA\n",
    "    if data_title:\n",
    "        np.savez(data_title, d_vals=d_vals, final_error = final_error_es)\n",
    "        \n",
    "    return final_error\n",
    "    \n",
    "# To run this cell uncomment the line below. Warning: run at own risk. Reduce some parametres to reduce rumtime.\n",
    "# ###final_error = final_error_sim([i**2 for i in range(2, 22)], maxiter = 1000), data_title = \"###2f.npz\")\n",
    "\n",
    "# Load data from commented line and make plot\n",
    "data = np.load(\"2f.npz\")\n",
    "\n",
    "txt = f\"$Figure$ XX: Frobenius norm $||A-WH||_F$ after 1000 iterations for different $d$.\"\n",
    "plot_function([[data['d_vals']]], [[data['final_error']]], [[\"tab:blue\"]], \n",
    "        fig_size=(15, 5), xlabels=[r\"$d$\"], ylabels=[r\"$||A-WH||_F$\"],\n",
    "        fig_text=txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2f) discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Task 3) Denoising <a class=\"anchor\" id=\"denoising\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add noise ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the noise as:\n",
    "\n",
    "$$A_{\\textrm{noisy}} = A + \\sigma E,$$\n",
    "\n",
    "where $\\sigma > 0 $ is our *noise level*, and $E$ sampled from $\\mathcal{N}(0, 1)$. Below is the handed out function to add noise to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(imgs_reshaped, sigma = 0.1, seed = 4269):\n",
    "    \"\"\"\n",
    "    Adds gaussian noise to images as described in text.\n",
    "    Note that imgs must be given as a (24*24*3, N) numpy array, i.e the reshaped images\n",
    "    Input:\n",
    "        imgs_reshaped: (1728,N) numpy array\n",
    "        sigma: scalar, noise level\n",
    "    Output:\n",
    "        noisy_faces: (1728,N) numpy array containing noisy images\n",
    "    \"\"\"\n",
    "    # Array that will store the rgb channels of the noisy images\n",
    "    noisy_faces = np.copy(imgs_reshaped)\n",
    "\n",
    "    # Number of noisy values we need\n",
    "    nnzero = imgs_reshaped[np.nonzero(imgs_reshaped)].shape[0]\n",
    "\n",
    "    # Sample noisy values and add noise\n",
    "    np.random.seed(seed)\n",
    "    noise = np.random.normal(0.0,1,nnzero)\n",
    "    noisy_faces[np.nonzero(imgs_reshaped)] += sigma*noise\n",
    "\n",
    "    # Clip to lie between 0 and 1 so that we can still interpret them as images\n",
    "    noisy_faces = np.maximum(0.0,np.minimum(1.0, noisy_faces))\n",
    "\n",
    "    return noisy_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vi kan nå legge til støy til bildene våre, legge på opasitetskanalaen igjen og plotte. Dere må gjøre noe lignende etter dere har anvendt NMF og ønsker å plotte rekonstruksjoner med opasitet.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a)\\ \n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to images\n",
    "faces500_reshaped_noisy = add_noise(faces500_reshaped, sigma = 0.1)\n",
    "\n",
    "# Calculate the error of the noisy images\n",
    "noise500_error = np.linalg.norm(faces500_reshaped_noisy - faces500_reshaped, 'fro') \n",
    "print(noise500_error)\n",
    "\n",
    "# For plotting noisy images we add the opacity\n",
    "# faces500_noisy = merge_and_reshape(faces500_reshaped_noisy, opacity500)\n",
    "# plotimgs(faces500_noisy, filename = \"3a_noisy_punks.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=3a_noisy_punks.png alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF -> ADD COOLER TITLEs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b) \n",
    "\n",
    "***• b) Using the same approach as task 2, fit an NMF using d = 64 to the noisy images.\n",
    "Plot the columns of W and the reconstructions like you did in 2c) and 2d). Are\n",
    "the reconstructions less noisy than the noisy images? Compare the columns of W\n",
    "and the reconstructions to the results you got in task 2.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 64\n",
    "W_noisy, H_noisy, err_noisy = NMF(faces500_reshaped_noisy, d, maxiter = 1000) # øke maxiter!!\n",
    "print(noise500_error)\n",
    "print(np.linalg.norm(np.dot(W_noisy,H_noisy) - faces500_reshaped, 'fro'))\n",
    "\n",
    "# W_columns_noisy = merge_and_reshape(W_noisy)\n",
    "# plotimgs(W_columns_noisy, 8, filename=\"3b_punks_basis_noisy.png\", figsize = (12, 12))\n",
    "\n",
    "# images_noisy_reborn = merge_and_reshape(np.dot(W_noisy, H_noisy), opacity500)\n",
    "# plotimgs(images_noisy_reborn, 8, filename=\"3b_noisy_reconstructions.png\", figsize = (12, 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c) \n",
    "\n",
    "***c) As you did in task 2, fit an NMF to the noisy images for a wide range of d,\n",
    "calculate the reconstruction error ∥A − W H∥F (important: A is here the original\n",
    "images, NOT the noisy images). Plot this with the error you calculated in 2f),\n",
    "where noiseless images were used. If you have a large enough range of d and large\n",
    "enough noise you should see that the reconstruction error for the noisy images\n",
    "is somewhere between ”U”-shaped and ”L”-shaped, and at some point the error\n",
    "should increase for large enough d. Explain why the plot looks like this (try to\n",
    "include the words ”underfitting” and ”overfitting”). Approximately, what value of\n",
    "d is the ”best fit”, that provides the lowest reconstruction error?***\n",
    "\n",
    "For an $m\\times n$-matrix we want to choose a value for d such that the total information is reduced:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "mn &< md+dn \\\\\n",
    "d &< \\frac{mn}{m+n} \n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIG SIMULATION TIME\n",
    "\n",
    "\n",
    "def denoise_at_different_N_sim(N_vals, maxiter, noise_level, number_of_d, filetitle = \"denoise_at_diff_N.npz\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize arrays for data storage\n",
    "    d_val_arrays         = []\n",
    "    final_errors         = []\n",
    "    final_errors_noisy   = []\n",
    "    initial_noise_errors = []\n",
    "    \n",
    "    for N in tqdm(N_vals):\n",
    "        # We consider only d values that reduce the amount of data\n",
    "        d_vals = np.linspace(64, np.floor(1728*N/(1728+N)), number_of_d, dtype = 'int')\n",
    "\n",
    "        # Load, reshape and add noise to images\n",
    "        faces_es                = load_images(N)\n",
    "        faces_reshaped_es       = split_and_reshape(faces_es)[0] # only want color channels\n",
    "        faces_reshaped_noisy_es = add_noise(faces_reshaped_es, noise_level)\n",
    "        \n",
    "        print(np.linalg.norm(faces_reshaped_noisy_es - faces_reshaped_es, 'fro')/N**(1/2))      \n",
    "        print(np.linalg.norm(faces_reshaped_noisy_es - faces_reshaped_es, 'nuc')/N**(1/2))      \n",
    "        \n",
    "        # Initialise arrays for final errors\n",
    "        final_error_es       = np.zeros(len(d_vals))\n",
    "        final_error_noisy_es = np.zeros(len(d_vals))\n",
    "    \n",
    "        # Do NMF for all the d values\n",
    "        for i in tqdm(range(len(d_vals)), leave = False):\n",
    "            # NMF with noise removal\n",
    "            W_n, H_n, err_n         = NMF(faces_reshaped_noisy_es, d_vals[i], maxiter = maxiter)\n",
    "            final_error_noisy_es[i] = np.linalg.norm(faces_reshaped_es - np.dot(W_n, H_n), 'fro')\n",
    "\n",
    "            # NMF without noise removal\n",
    "            W, H, err         = NMF(faces_reshaped_es, d_vals[i], maxiter = maxiter)    \n",
    "            final_error_es[i] = err[-1]\n",
    "        \n",
    "        # Add the errors to the final_errors\n",
    "        d_val_arrays.append(d_vals)\n",
    "        final_errors.append(final_error_es)\n",
    "        final_errors_noisy.append(final_error_noisy_es)\n",
    "        initial_noise_errors.append(np.linalg.norm(faces_reshaped_noisy_es - faces_reshaped_es, 'fro'))        \n",
    "    \n",
    "    # Save the data\n",
    "    np.savez(filetitle, N_vals = N_vals, maxiter = maxiter, noise_level = noise_level, d_val_arrays=d_val_arrays, \n",
    "             final_errors = final_errors, final_errors_noisy = final_errors_noisy, initial_noise_errors = initial_noise_errors)\n",
    "    \n",
    "# We load N images, with N chosen from this array\n",
    "N_vals      = [250, 500, 750, 1000]\n",
    "maxiter     = 1000\n",
    "noise_level = 0.1\n",
    "number_of_d = 100\n",
    "\n",
    "# Run the simulation at own risk, remove the overwrite protection \"###\"\n",
    "# ###denoise_at_different_N_sim(N_vals, maxiter, noise_level, number_of_d, filetitle = \"###denoise_at_diff_N.npz\")\n",
    "\n",
    "\"\"\"\n",
    "N_vals      = [10, 15, 20, 25]\n",
    "maxiter     = 10\n",
    "noise_level = 0.1\n",
    "number_of_d = 10\n",
    "\n",
    "denoise_at_different_N_sim(N_vals, maxiter, noise_level, number_of_d, filetitle = \"test_denoise_at_diff_N.npz\")\n",
    "\"\"\"\n",
    "\n",
    "data = np.load(\"denoise_at_diff_N.npz\")\n",
    "d_val_arrays       = data['d_val_arrays']\n",
    "final_errors       = data['final_errors']\n",
    "final_errors_noisy = data['final_errors_noisy']\n",
    "initial_noise_errors = data['initial_noise_errors']\n",
    "\n",
    "for i in range(4):\n",
    "    final_errors[i] /= N_vals[i]**(1/2)\n",
    "    final_errors_noisy[i] /= N_vals[i]**(1/2)\n",
    "    initial_noise_errors[i] /= N_vals[i]**(1/2)\n",
    "\n",
    "x_vals_list = [[d_val_arrays[i], d_val_arrays[i],       [d_val_arrays[i][0],      d_val_arrays[i][-1]]]     for i in range(4)]\n",
    "y_vals_list = [[final_errors[i], final_errors_noisy[i], [initial_noise_errors[i], initial_noise_errors[i]]] for i in range(4)]\n",
    "linestyles  = [[\"g\", \"b\", \"b--\"] for i in range(4)]\n",
    "fig_text    = f\"$Figure$ XX: fdsdfdgs\"\n",
    "\n",
    "plot_function(x_vals_list, y_vals_list, linestyles, fig_size = (15, 10), shape = (2, 2),\n",
    "        xlabels = [\"\", \"\", f\"$d$\", f\"$d$\"], ylabels = [r\"$\\frac{1}{\\sqrt{N}}\\cdot||A-WH||_F$\", \"\", r\"$\\frac{1}{\\sqrt{N}}\\cdot||A-WH||_F$\", \"\"],\n",
    "        fig_text=fig_text, fig_text_size = 15, fig_text_y = 0.04, \n",
    "        title_size = 15, titles = [f\"$N=${N}\" for N in N_vals], \n",
    "        label_size = 15, labels = [False, [r\"$\\sigma=0$\", r\"$\\sigma=0.1$\", \"Noise error\"], False, False],\n",
    "        loc = [False, \"lower left\", False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***EXPLAIN division by sqrt(N). ***\n",
    "\n",
    "We want a measure of the error per image. The division by $\\sqrt{N}$ solves this:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    ||A_{M\\times N}||_F = \\sqrt{\\sum_j^N{\\sum_i^M{a_{i,j}^2}}} \n",
    "                        = \\sqrt{\\sum_j^N{{||\\vec a_{j}||_F^2}}} \n",
    "                        = \\sqrt{N\\cdot \\overline {{||\\vec a||_F^2}}} \n",
    "                        = \\sqrt{N}\\sqrt{\\cdot \\overline {{||\\vec a||_F^2}}}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus dividing $||A_{M\\times N}||_F$ gives us a more just comparison.\n",
    "\n",
    "***Fix above***\n",
    "\n",
    "\n",
    "We will now do a sligthly different experiment with the noisy images. If we had done the same analysis as previously, the difference between the noisy A, $A_n$, and the NMF, $WH$, would decrease as in FIGUREERROR. Comparing the NMF with the original matrix $A$, however, we will (hopefully) find NMF's that are closer to $A$ than the original noisy matrix. This would mean that the details the NMF 'loses' for smaller values of d actually corresponds to the noise, resulting in denoised images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_at_different_noise_lvl_sim(noise_levels, maxiter, d_vals, N = 500, filetitle = \"denoise_at_diff_noise.npz\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Load and reshape N images \n",
    "    faces_es          = load_images(N)\n",
    "    faces_reshaped_es = split_and_reshape(faces_es)[0]\n",
    "    \n",
    "    # Initialize lists for errors\n",
    "    final_errors_noisy   = []\n",
    "    initial_noise_errors = []\n",
    "    \n",
    "    # Do NMF for each noise level\n",
    "    for noise_level in tqdm(noise_levels, leave=False):\n",
    "        # Add noise to images\n",
    "        faces_reshaped_noisy_es = add_noise(faces_reshaped_es, noise_level)\n",
    "\n",
    "        # Initialise arrays for final errors\n",
    "        final_err = np.zeros(len(d_vals))\n",
    "\n",
    "        # Do NMF algorithm for each d-value\n",
    "        for index, d_val in tqdm(enumerate((d_vals)), total = len(d_vals), leave=False):\n",
    "            W, H, err    = NMF(faces_reshaped_noisy_es, d_val, maxiter = maxiter)\n",
    "            final_err[index] = np.linalg.norm(faces_reshaped_es - np.dot(W, H), 'fro')\n",
    "\n",
    "        # Add values\n",
    "        final_errors_noisy.append(final_err)\n",
    "        initial_noise_err = np.linalg.norm(faces_reshaped_noisy_es - faces_reshaped_es, 'fro')\n",
    "        initial_noise_errors.append(initial_noise_err)\n",
    "        \n",
    "        # Save and plot \n",
    "        np.savez(f\"denoise_at_sigma={noise_level}\", N=N, maxiter=maxiter, noise_level=noise_level, d_vals=d_vals, \n",
    "                 final_err = final_err, initial_noise_err = initial_noise_err)\n",
    "        plt.plot(d_vals, final_err)\n",
    "        plt.axhline(y=initial_noise_err)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    # SAVE DATA\n",
    "    np.savez(filetitle, N=N, maxiter=maxiter, noise_levels=noise_levels, d_vals=d_vals, \n",
    "             final_errors_noisy = final_errors_noisy, initial_noise_errors = initial_noise_errors)\n",
    "    \n",
    "# Set parametres\n",
    "\n",
    "######## ØKES TIL maxiter 1000, 100? dvals\n",
    "\n",
    "noise_levels = [0, 0.05, 0.1, 0.2]\n",
    "maxiter      = 1000\n",
    "d_vals       = np.linspace(64, 1000, 100, dtype = 'int')\n",
    "\n",
    "# Do simulation at own risk\n",
    "# ###denoise_at_different_noise_lvl_sim(noise_levels, maxiter, d_vals, N = 500, filetitle = \"###denoise_at_diff_noise.npz\")\n",
    "    \n",
    "# Load data\n",
    "data = np.load(\"denoise_at_diff_noise.npz\")\n",
    "        \n",
    "x_vals_list = [data['d_vals'] for i in range(len(noise_levels))]\n",
    "y_vals_list = []\n",
    "for i, noise_level in enumerate(noise_levels):\n",
    "    y_vals_list.append(data['final_errors_noisy'][i])\n",
    "    y_vals_list.append([data[initial_noise_errors[i]], data[initial_noise_errors[i]]])\n",
    "\n",
    "    \n",
    "linestyles  = [[\"b\", \"g\", \"g--\"] for i in range(4)]\n",
    "fig_text    = \"Figure XX: fdsdfdgs\"\n",
    "\n",
    "plot_function([x_vals_list], [y_vals_list]\n",
    "              , linestyles, fig_size = (15, 10), shape = (2, 2),\n",
    "        xlabels = [\"d\"], ylabels = [r\"log $||A-WH||_F$\", \"\", r\"log $||A-WH||_F$\", \"\"],\n",
    "        fig_text=fig_text, fig_text_size = 15, fig_text_y = 0.04, \n",
    "        title_size = 15, titles = [f\"$N=${N}\" for N in N_vals], \n",
    "        label_size = 15, labels = [False, [r\"$\\sigma=0$\", r\"$\\sigma=0.1$\", \"Noise error\"], False, False],\n",
    "        loc = [False, \"lower left\", False, False])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***TODO: At what d is error lowest***\n",
    "\n",
    "\n",
    "\n",
    "This simulation for different values of noise (characterized by the value $\\sigma$), can be seen in FIGUREFARGEHELVETE. The horizontal dotted lines are the frobenius differences between the original matrix and the different noisy matrices.\n",
    "\n",
    "Why the algorithm works, and why the graphs are shaped the way they are can, at least partially, be explained with the terms underfitting and overfitting. The phenomena of underfitting and overfitting are major concerns in the field of machine learning, and can be explained with a classic example from machine learning. Say you would like to make an algorithm that can recognize handwritten digits. To do this, you can use a neural network combined with a very large dataset. The neural network works as a function approximator, that approximates the function image -> digit using the dataset. \n",
    "\n",
    "Underfitting happens when the dataset or the number of training epochs over the dataset is too small. With too little information, the neural network cannot approximate the real (and rather complex) function of digit recognition, as the nuances of the problem hasn't manifested themselves yet. This is like trying to discern a non-linear relationship between two parameters, having only two datapoints. Too little information results in a function approximation of a simpler function, leading to large errors.\n",
    "\n",
    "Overfitting on the other hand, happens when the neural network trains too much on the same dataset. The function approximation will then begin to take into account the individual aspects of the images, rather than the general notion of what a digit is. The neural network would be able to predict almost perfectly which numbers the dataset corresponds to, but has now learned features of numbers that are specific to the dataset, and not features of general numbers. This, too, results in large errors.\n",
    "\n",
    "When the above algorithm reconstructs the noisy images with a low value of d, the drastically smaller amount of information leads to underfitting, with corresponding large errors. Underfitting is even more predominant than the noise itself, as can be seen by comparing the values on the graph towards the left with the horizontal lines. It is therefore necessary to increase the d to get a good reconstruction of A. \n",
    "\n",
    "Too large a value of d will, however, result in columns of $W$ corresponding to features specific to the noisy images, rather than general features of faces. The noisy images does still have features like 'eye placement' and'hair color', but they will also have features originating from the added noise. The clue, then, is to increase the value of d until the features in $W$ corresponds to general features of the faces, and stop before the features that are specific to the noisy dataset (i.e. the noise) are added. \n",
    "\n",
    "These effects results in large errors for small d, and large errors for large d. It makes sense therefore, that the graph has a critical value of d with the smallest frobenius-difference with the noiseless image A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3d) \n",
    "\n",
    "***d) Include a suitable conclusion to the entire project. What did you find? What\n",
    "are the advantages and disadvantages of applying NMF for images, in particular\n",
    "for denoising?***\n",
    "\n",
    "Random\n",
    "\n",
    "piss\n",
    "\n",
    "på \n",
    "\n",
    "konk"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
